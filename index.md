---
layout: default
---

# Overview

Sponsored by the Center for Human-Compatible AI at UC Berkeley, and with support from the Simons Institute and the Center for Long-Term Cybersecurity, we are convening a cross-disciplinary group of researchers to examine the near-term policy concerns of Reinforcement Learning (RL).
RL is a rapidly growing branch of AI research, with the capacity to learn to exploit our dynamic behavior in real time.
From YouTube’s recommendation algorithm to post-surgery opioid prescriptions, RL algorithms are poised to permeate our daily lives.
The ability of the RL system to tease out behavioral responses, and the human experimentation inherent to its learning, motivate a range of crucial policy questions about RL’s societal implications that are distinct from those addressed in the literature on other branches of Machine Learning (ML).

We began addressing these issues as part of the [2020 Simons Institute program on the Theory of Reinforcement Learning](https://simons.berkeley.edu/news/mapping-political-economy-reinforcement-learning-systems-case-autonomous-vehicles), and throughout 2020/21 we have been broadening the discussion through an [ongoing reading group](https://geesegraduates.org/2020/10/26/political-economy-of-reinforcement-learning/), including perspectives from Law and Policy.
The aim of this workshop will be to establish a common language around the state of the art of RL across key societal domains.
From this examination, we hope to identify specific interpretive gaps that can be elaborated or filled by members of our community.
Our ultimate goal will be to map near-term societal concerns and indicate possible cross-disciplinary avenues towards addressing them.

# Call for Papers

We are inviting papers from the fields of computer science, governance, law, economics, and game theory within the following research tracks:

<ol class="cfp-list">
    <li>
    <b>Vagueness and specification.</b>
    RL methods require a specification of actions, observations, and rewards which condense vague real-world systems into a form suitable for our algorithms.  However, this specification makes impactful choices which give an underlying sense of normative indeterminacy -- the lack of prior standards or guidelines for how a given system ought to perform in simulation or post-deployment. Technical and policy discussions related to optimization and control often fail to address this more basic, irreducibly sociotechnical problem. The saliency of vagueness is likely to increase as RL capabilities expand, allowing designers to intervene at scales that were previously inaccessible even to governments and corporations. The following questions seem in scope: How might designers make sense of emergent system behaviors that are difficult to evaluate? Whose expertise or judgment is needed to better evaluate such behaviors either within or across domains? In what ways and contexts can the task of RL specification actually help refine the understanding of a given problem domain?
    </li>
    <li>
    <b>Legitimacy, Accountability, and Feedback.</b>
    The maturation of RL naturally raises questions about who should have the ability to oversee these systems and evaluate performance over time. In terms of how an agent learns to navigate a given environment proficiently, RL may in fact automate many feedback mechanisms for oversight and control that governmental institutions presently enforce or take for granted. Such determinations lie at the heart of modern conceptions of political sovereignty. We ask: How might preferred optimization techniques bear on existing normative concerns (e.g. data privacy)? To what extent do stakeholders have a voice in system evaluation? How can choices about how to structure computation be brought into alignment with legal conceptions of rights and duties?
    </li>
    <li>
    <b>Tools for democratization.</b>
    There is growing concern that the successful and safe integration of advanced AI systems into human societies will require democratic control and oversight of those systems themselves. At present there are many proposed standards, guidelines, laws, governance mechanisms, and regulations that could be leveraged to enact such democratization, as well as organizational theories (e.g. distributed expertise) that suggest how it should proceed. But it remains unclear what forms of inquiry, evaluation, and control are warranted or ought to be prioritized to deal with the challenges of RL in particular domains. We invite papers that propose new policy tools (technical or otherwise) in the context of RL, as well as explicitly normative contributions to the ongoing debate about what is at stake in democratization.
    </li>
</ol>

We encourage a broad interpretation of the problem spaces outlined in this call, and would urge researchers broadly interested in the topic of RL and society to consider submitting a contribution, even if it does not explicitly fall within the topics proposed above.

## Key Dates

<ul>
    <li>Paper submissions due: Sep 18, 2021, <a target="blank" href="https://www.timeanddate.com/time/zones/aoe">AoE</a></li>
    <li>Accept/Reject Notification: Oct 23, 2021, <a target="blank" href="https://www.timeanddate.com/time/zones/aoe">AoE</a></li>
</ul>

## Submission Instructions

Submitted papers must conform to the <a href="https://neurips.cc/Conferences/2021/CallForPapers" target="blank">NeurIPS 2021 formatting instructions</a>.

Paper submission will be through OpenReview (link TBA).

# How to participate

In addition to submitting a paper as per the CFP above, you can also participate in this workshop as follows:

 * Register for workshop updates below
 * Check this site closer to NeurIPS for more updates

<iframe src="https://docs.google.com/forms/d/e/1FAIpQLSdSOu8dPtBHHG126JJ5ns3TFVj4c_bnZiZflOcmG31h9l55xQ/viewform?embedded=true" width="100%" height="600pt" frameborder="0" marginheight="0" marginwidth="0">Loading…</iframe>
